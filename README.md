# Airlake Dag Factory
Dynamically build Airflow DAGs from YAML files

**Airlake Factory Tool**

![tool](./assets/dag_factory.png)


## Install Airlake Factory Package

**Must Have**
```
export PYTHONPATH=$PYTHONPATH:/airfactory
```
**Run Test**
```
make test
```
## Build Lib
```
make dist
```
## Usage This Lib
**Copy airfactory-1.0.0.tar.gz in dist folder to your project**
**Configure requirements.txt**
```
./packages/airfactory-1.0.0.tar.gz
```

**How To Work**

1. Create a YAML configuration file called config_file.yml and save it within your dags folder:
```yaml
default_args:
  owner: nguyenvantuan140397@gmail.com
  start_date: 2024-06-28
schedule: "0 2 * * *"
tasks:
  score_products:
    operator: BigQueryOperator
    sql: SELECT * FROM `dwh.product.vw_product_fraud_score`
    destination_dataset_table: "dwh.product.product_scoring_{{ macros.localtz.ds_nodash(ti) }}"

  update_realtime_cached:
    operator: BigqueryToSheetOperator
    sheet_url: "product.product_scores"
    sql: |
      SELECT CAST(sku AS STRING) sku, product_fraud_score_new score
      FROM `dwh.product.vw_product_fraud_score_update`
      WHERE sku IS NOT NULL
    dependencies: ["score_products"]
```
2. In the same folder, create a python file called generate_dags.py. This file is responsible for generating the DAGs from the configuration file and is a one-time setup. You won't need to modify this file unless you want to add more configuration files or change the configuration file name.
```python
'''
Generated by Airflow Datalake! Do not edit!
author: tuancamtbtx
Timestamp 2024-07-19T17:09:31.157517+07:00
'''

from airflow import DAG

from airfactory.render import load_dag

conf = {'schedule_interval': '0 0 * * *', 'timetable': {'start_date': '2021-01-01', 'end_date': '2021-01-02'}, 'default_args': {'owner': 'airflow', 'depends_on_past': False, 'email_on_failure': False, 'email_on_retry': False, 'retries': 1}, 'tasks': {'task1': {'operator': 'airflow.operators.bash.BashOperator', 'bash_command': 'echo 1', 'execution_timeout': 60, 'sla': 30}, 'task2': {'operator': 'airflow.operators.bash.BashOperator', 'bash_command': 'echo 2', 'execution_timeout': 60, 'sla': 30}}}
name = 'test_dag'
load_dag(globals(), name, conf)
```

## Benefits
**Design Principle:**
- Sync a table from MySQL/PostgreSQL in 10 lines of config
- Config should be clean, users just need to write minimal key to make a dag run
- Non-Tech users could write it without thinking about programming complex
- Alerting & Retrying must be enable by default
- RBAC Authorization must be provided.
- Users can backfill data when they want, via an UI.

**Runtime Cost**
- This system should be fault-tolerance
- Reduce 70% Cost by Running preemptible VMs